{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>Do you know the difference between $1 million ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interview.txt</td>\n",
       "      <td>Many entrepreneurs start a business with grand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>Section I: How We Got Here\\nIn Section I of $1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                               text\n",
       "0    article.txt  Do you know the difference between $1 million ...\n",
       "1  interview.txt  Many entrepreneurs start a business with grand...\n",
       "2    summary.txt  Section I: How We Got Here\\nIn Section I of $1..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a list of all the text files in the data folder\n",
    "text_data: List[Dict[str, str]] = []\n",
    "\n",
    "for filename in os.listdir('./data'):\n",
    "\n",
    "    # Read and store the text of each \"txt\" file in the text_data list\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join('data', filename), 'r') as f:\n",
    "            text = f.read()\n",
    "            text_data.append({'filename': filename, 'text': text})\n",
    "\n",
    "# Convert the text_data list into a DataFrame\n",
    "df = pd.DataFrame(text_data)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chunks(lst: List[Any], n: int):\n",
    "    \"\"\"\n",
    "    Divides a list into chunks of size n. \n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def count_tokens(text: str, tokenizer: GPT2TokenizerFast) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in the text.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3236 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3236\n",
      "1    1409\n",
      "2    5460\n",
      "Name: text, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>Do you know the difference between 1 million a...</td>\n",
       "      <td>503</td>\n",
       "      <td>[5211, 345, 760, 262, 3580, 1022, 352, 1510, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>was two to four years of management consulting...</td>\n",
       "      <td>498</td>\n",
       "      <td>[9776, 734, 284, 1440, 812, 286, 4542, 18158, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>business got complicated and after a while the...</td>\n",
       "      <td>512</td>\n",
       "      <td>[22680, 1392, 8253, 290, 706, 257, 981, 262, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>when he started his first business No But was ...</td>\n",
       "      <td>496</td>\n",
       "      <td>[12518, 339, 2067, 465, 717, 1597, 1400, 887, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>project or career path before they ve given it...</td>\n",
       "      <td>487</td>\n",
       "      <td>[16302, 393, 3451, 3108, 878, 484, 1569, 1813,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>article.txt</td>\n",
       "      <td>skills that make the duo better together compa...</td>\n",
       "      <td>273</td>\n",
       "      <td>[8135, 2171, 326, 787, 262, 18545, 1365, 1978,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interview.txt</td>\n",
       "      <td>Many entrepreneurs start a business with grand...</td>\n",
       "      <td>512</td>\n",
       "      <td>[7085, 17038, 923, 257, 1597, 351, 4490, 3352,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>interview.txt</td>\n",
       "      <td>premium and over delivering to satisfy the cli...</td>\n",
       "      <td>512</td>\n",
       "      <td>[31605, 1505, 290, 625, 13630, 284, 15959, 262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>interview.txt</td>\n",
       "      <td>the name of a promotion to something memorable...</td>\n",
       "      <td>204</td>\n",
       "      <td>[1169, 1438, 286, 257, 12148, 284, 1223, 18078...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>Section I How We Got Here In Section I of 100M...</td>\n",
       "      <td>475</td>\n",
       "      <td>[16375, 314, 1374, 775, 11853, 3423, 554, 7275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>compared to any other product or service avail...</td>\n",
       "      <td>449</td>\n",
       "      <td>[5589, 1144, 284, 597, 584, 1720, 393, 2139, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>buying Avoid lowering prices to improve the pr...</td>\n",
       "      <td>471</td>\n",
       "      <td>[11110, 1112, 24390, 21683, 4536, 284, 2987, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>you to leave a review of 100M Offers if you ha...</td>\n",
       "      <td>475</td>\n",
       "      <td>[5832, 284, 2666, 257, 2423, 286, 1802, 44, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>you have is good Monetize Flow Get the prospec...</td>\n",
       "      <td>449</td>\n",
       "      <td>[5832, 423, 318, 922, 2892, 316, 1096, 27782, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>is unique differentiated and unable to be comp...</td>\n",
       "      <td>469</td>\n",
       "      <td>[271, 3748, 47543, 290, 5906, 284, 307, 3688, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>colors designs sizes etc You must sell out con...</td>\n",
       "      <td>460</td>\n",
       "      <td>[4033, 669, 9824, 10620, 3503, 921, 1276, 3677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>or provides value Prove that each bonus provid...</td>\n",
       "      <td>458</td>\n",
       "      <td>[273, 3769, 1988, 1041, 303, 326, 1123, 7202, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>and airfare if no value Wage Payment â pay the...</td>\n",
       "      <td>466</td>\n",
       "      <td>[392, 1633, 9496, 611, 645, 1988, 45874, 28784...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>summary.txt</td>\n",
       "      <td>the headline or the wrapper of your offer Chan...</td>\n",
       "      <td>138</td>\n",
       "      <td>[1169, 16534, 393, 262, 29908, 286, 534, 2897,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename                                               text  \\\n",
       "0     article.txt  Do you know the difference between 1 million a...   \n",
       "1     article.txt  was two to four years of management consulting...   \n",
       "2     article.txt  business got complicated and after a while the...   \n",
       "3     article.txt  when he started his first business No But was ...   \n",
       "4     article.txt  project or career path before they ve given it...   \n",
       "5     article.txt  skills that make the duo better together compa...   \n",
       "6   interview.txt  Many entrepreneurs start a business with grand...   \n",
       "7   interview.txt  premium and over delivering to satisfy the cli...   \n",
       "8   interview.txt  the name of a promotion to something memorable...   \n",
       "9     summary.txt  Section I How We Got Here In Section I of 100M...   \n",
       "10    summary.txt  compared to any other product or service avail...   \n",
       "11    summary.txt  buying Avoid lowering prices to improve the pr...   \n",
       "12    summary.txt  you to leave a review of 100M Offers if you ha...   \n",
       "13    summary.txt  you have is good Monetize Flow Get the prospec...   \n",
       "14    summary.txt  is unique differentiated and unable to be comp...   \n",
       "15    summary.txt  colors designs sizes etc You must sell out con...   \n",
       "16    summary.txt  or provides value Prove that each bonus provid...   \n",
       "17    summary.txt  and airfare if no value Wage Payment â pay the...   \n",
       "18    summary.txt  the headline or the wrapper of your offer Chan...   \n",
       "\n",
       "    num_tokens                                             tokens  \n",
       "0          503  [5211, 345, 760, 262, 3580, 1022, 352, 1510, 2...  \n",
       "1          498  [9776, 734, 284, 1440, 812, 286, 4542, 18158, ...  \n",
       "2          512  [22680, 1392, 8253, 290, 706, 257, 981, 262, 5...  \n",
       "3          496  [12518, 339, 2067, 465, 717, 1597, 1400, 887, ...  \n",
       "4          487  [16302, 393, 3451, 3108, 878, 484, 1569, 1813,...  \n",
       "5          273  [8135, 2171, 326, 787, 262, 18545, 1365, 1978,...  \n",
       "6          512  [7085, 17038, 923, 257, 1597, 351, 4490, 3352,...  \n",
       "7          512  [31605, 1505, 290, 625, 13630, 284, 15959, 262...  \n",
       "8          204  [1169, 1438, 286, 257, 12148, 284, 1223, 18078...  \n",
       "9          475  [16375, 314, 1374, 775, 11853, 3423, 554, 7275...  \n",
       "10         449  [5589, 1144, 284, 597, 584, 1720, 393, 2139, 1...  \n",
       "11         471  [11110, 1112, 24390, 21683, 4536, 284, 2987, 2...  \n",
       "12         475  [5832, 284, 2666, 257, 2423, 286, 1802, 44, 32...  \n",
       "13         449  [5832, 423, 318, 922, 2892, 316, 1096, 27782, ...  \n",
       "14         469  [271, 3748, 47543, 290, 5906, 284, 307, 3688, ...  \n",
       "15         460  [4033, 669, 9824, 10620, 3503, 921, 1276, 3677...  \n",
       "16         458  [273, 3769, 1988, 1041, 303, 326, 1123, 7202, ...  \n",
       "17         466  [392, 1633, 9496, 611, 645, 1988, 45874, 28784...  \n",
       "18         138  [1169, 16534, 393, 262, 29908, 286, 534, 2897,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the text and count the number of tokens in each file\n",
    "print(df['text'].apply(lambda x: count_tokens(x, tokenizer)))\n",
    "\n",
    "# Go through each row of the Dataframe and tokenize the text. Split it into\n",
    "# chunks of 512 tokens and store the chunks in a list\n",
    "tokenized_chunks = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text: List[int] = tokenizer.encode(row.text)\n",
    "    text_words: List[str] = re.findall(r'\\w+', row.text)\n",
    "\n",
    "    # Get words/token ratio\n",
    "    num_tokens = len(tokenized_text)\n",
    "    num_words = len(text_words)\n",
    "    words_token_ratio = num_words / num_tokens\n",
    "\n",
    "    # We want 512 tokens per chunk, so we get how many words is that by multiplying the\n",
    "    # number of tokens by the words/token ratio. We add 15% to the number of words, because\n",
    "    # the words/token ratio underestimates the number of words in the text for a set number\n",
    "    # of tokens.\n",
    "    words_per_chunk = int(512 * words_token_ratio * 1.15)\n",
    "\n",
    "    # Split the tokenized text into chunks of 512 approximately 512 tokens\n",
    "    chunks = list(divide_chunks(text_words, words_per_chunk))\n",
    "\n",
    "    # Join the words in each chunk into a string\n",
    "    for chunk in chunks:\n",
    "        chunk_sentence = ' '.join(chunk)\n",
    "        tokenized_chunk = tokenizer.encode(\n",
    "            chunk_sentence,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        tokenized_chunks.append({\n",
    "            'filename': row.filename,\n",
    "            'text': chunk_sentence,\n",
    "            'num_tokens': len(tokenized_chunk),\n",
    "            'tokens': tokenized_chunk,\n",
    "        })\n",
    "\n",
    "token_data = pd.DataFrame(tokenized_chunks)\n",
    "token_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file with the OpenAI API key\n",
    "# Note: Dont forget to have a .env file in \"ansible/storage\" with the OPENAI_API_KEY\n",
    "load_dotenv(dotenv_path=Path('../ansible/storage/.env'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize a list to store the embeddings\n",
    "embeddings: List[Tuple[str, List[float], Dict[str, Any]]] = []\n",
    "\n",
    "# Go through each row of the token_data DataFrame and create an embedding for each chunk\n",
    "for row in token_data.itertuples():\n",
    "\n",
    "    # Create the embedding. This will generate a vector of 1536 dimensions for each chunk\n",
    "    response = openai.Embedding.create(\n",
    "        input=row.text,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )\n",
    "\n",
    "    # Fetch the embedding vector\n",
    "    vector = response['data'][0]['embedding']\n",
    "\n",
    "    # Store the embedding\n",
    "    embeddings.append((\n",
    "        f\"vector_{row.filename}_{row.Index}\",\n",
    "        vector,\n",
    "        {'filename': row.filename, 'text': row.text}\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')\n",
    "\n",
    "# Raise an error if the PINECONE_API_KEY or PINECONE_ENVIRONMENT are not set\n",
    "if PINECONE_API_KEY is None or PINECONE_ENVIRONMENT is None:\n",
    "    raise ValueError(\n",
    "        'Please set the PINECONE_API_KEY and PINECONE_ENVIRONMENT environment variables'\n",
    "    )\n",
    "\n",
    "# Get the dimensions of one of the embeddings\n",
    "embedding_dimensions = len(embeddings[0][1])\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT,\n",
    ")\n",
    "\n",
    "# Check if a \"hormozi-gpt\" index exists. If not, create it.\n",
    "index_name = 'hormozi-gpt'\n",
    "active_indexes = pinecone.list_indexes()\n",
    "if index_name not in active_indexes:\n",
    "    pinecone.create_index(name=index_name, dimension=embedding_dimensions)\n",
    "\n",
    "# Describe the index\n",
    "description = pinecone.describe_index(index_name)\n",
    "\n",
    "# Check that the index has the correct dimensions\n",
    "if description.dimension != embedding_dimensions:\n",
    "    raise ValueError(\n",
    "        f\"The index {index_name} has dimension {description.dimension}, but the embeddings have dimension {embedding_dimensions}.\"\n",
    "    )\n",
    "\n",
    "# Insert the embeddings into the index\n",
    "index = pinecone.Index(index_name=index_name)\n",
    "upsert_response = index.upsert(\n",
    "    vectors=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "populating_pinecone-Z1z3EglT-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
